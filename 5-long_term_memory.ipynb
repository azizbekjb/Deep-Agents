{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2cfb9882",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------START----------------------------------------\n",
    "# I have learned from LangchainDocs\n",
    "# Source link: https://docs.langchain.com/oss/python/deepagents/long-term-memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6ff51284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from deepagents import create_deep_agent\n",
    "from langgraph.store.memory import InMemoryStore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6714cb9a",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a40b014a",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = InMemoryStore()\n",
    "agent = create_deep_agent(\n",
    "    store=store,\n",
    "    use_longterm_memory=True,\n",
    "    model=\"google_genai:gemini-2.5-flash\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5377b1",
   "metadata": {},
   "source": [
    "# The /memories/ path convention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "618be50b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'text',\n",
       "  'text': 'I can do that. What content would you like to include in the draft?',\n",
       "  'extras': {'signature': 'CqsBAdHtim/cRa1Ol4rwmNoeGhBJQgmZrKT2H4uiudIoM/wDclpM4ejGFlXNMnVINuJJKE0Eaf90kgqKMnaTV63X0Q+auM7Baa6M/5itJ7NxwmEaboy5TlpLDSHi29jvHpz9SAsBpHkmeNnePyjG9NyPhYJCf1MaOqV7R7LgyuzT0IxmPTt+jymsK6xJPUew095nhSxL0npqpKvrgPrtZGI8Kcd06ri37NMeOp4u'}}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transient file (lost after thread ends)\n",
    "agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Write draft to /draft.txt\"}]\n",
    "})[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1c6afd50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There is no final report to save yet. Please tell me what you would like the report to contain.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Persistent file (survives across threads)\n",
    "agent.invoke({\n",
    "    \"messages\":[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Save final report to /memories/report.txt\"\n",
    "    }]\n",
    "})[\"messages\"][-1].text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c1d84f",
   "metadata": {},
   "source": [
    "# Cross-thread persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f403e483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What are my preferences ?', additional_kwargs={}, response_metadata={}, id='113d36d6-86de-4b00-8f02-ad4ec1013b65'),\n",
       "  AIMessage(content=[{'type': 'text', 'text': 'I do not have access to your personal preferences. I am a large language model, trained by Google, and do not store any user-specific information.', 'extras': {'signature': 'CskBAdHtim9AZzVowwCb8F/EFzZk327Rs0A5+z9NkgFfVAgR+CaA/jNT2LENrDDn/0x5mFIzbc3EeEsVuPl6s74LWwBiAbjdbAl021e9YAswKf7MzMZmFyd1fOmz8XbJnQrIx6sh2k+m2+AluizoHTcCaOBPydtpg6eYD27KCdwmZkyXPcLAtc9B9wKjuyIQdUInSYA9VrTYHzwfnMIDya33nFJ5modYgQxDuWSxX7sdcgjHFXny4zwF7te1iFsKptqrpE5RG9tFzCIn'}}], additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--d53e8936-3f1d-4bcb-b1aa-7eb482869e5b-0', usage_metadata={'input_tokens': 4531, 'output_tokens': 68, 'total_tokens': 4599, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 37}})]}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Thread 1: Write to long-term memory\n",
    "config1 = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}} \n",
    "agent.invoke({\n",
    "    \"messages\":[{\"role\": \"user\", \"content\": \"Save my preferenses to /memories/preferences.txt\"}]\n",
    "},\n",
    "config=config1)\n",
    "\n",
    "# Thread 2: Read from long-term memory \n",
    "config2 = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
    "agent.invoke({\n",
    "    \"messages\": [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What are my preferences ?\"\n",
    "    }]\n",
    "    },\n",
    "    config=config2\n",
    ")\n",
    "# Agent can read /memories/preferences.txt from the first thread"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4cc5d7",
   "metadata": {},
   "source": [
    "# Use cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4acb380",
   "metadata": {},
   "source": [
    "## User preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d9db0ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_deep_agent(\n",
    "    store=store,\n",
    "    use_longterm_memory=True,\n",
    "    system_prompt=\"\"\"When users tell you their preferences, save them to /memories/user_preferences.txt\n",
    "    so you remember them in future conversations.\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d70a2a",
   "metadata": {},
   "source": [
    "## Self-improving instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "654abd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_deep_agent(\n",
    "    store=store,\n",
    "    use_longterm_memory=True,\n",
    "    system_prompt=\"\"\"You have a file at /memories/instructions.txt with additonal\n",
    "    instructions and preferences\n",
    "    \n",
    "    Read this file at the start of conversations to understand user preferences.\n",
    "    \n",
    "    When users provide feedback like \"pleace always do X\" or \"I prefer X\",\n",
    "    update /memories/instructions.txt using the edit_file tool.\"\"\",\n",
    "    model=\"google_genai:gemini-2.5-flash\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc30c74",
   "metadata": {},
   "source": [
    "## Knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "43f23abe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Okay, I've saved the project notes to `/memories/project_notes.txt`. What would you like to do next?\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conversation 1: Learn about a project\n",
    "agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"We're building an agent with Langchain and deepagents. Save project notes.\"}]\n",
    "})[\"messages\"][-1].content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b13a436a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am a large language model, trained by Google. I do not use a framework in the way a software project would. I am the framework! '"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conversation 2: Use that knowledge\n",
    "agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"What framework are we using?\"}]\n",
    "})[\"messages\"][-1].content\n",
    "# Agent reads /memories/project_notes.txt from previous conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c79352",
   "metadata": {},
   "source": [
    "## Research projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7e8772be",
   "metadata": {},
   "outputs": [],
   "source": [
    "research_agent = create_deep_agent(\n",
    "    store=store,\n",
    "    use_longterm_memory=True,\n",
    "    system_prompt=\"\"\"You are a research assistant.\n",
    "\n",
    "    Save your research progress to /memories/research/:\n",
    "    - /memories/research/sources.txt - List of sources found\n",
    "    - /memories/research/notes.txt - Key findings and notes\n",
    "    - /memories/research/report.md - Final report draft\n",
    "\n",
    "    This allows research to continue across multiple sessions.\"\"\",\n",
    "    model=\"google_genai:gemini-2.5-flash\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
